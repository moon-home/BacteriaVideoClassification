{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import LSTM, Dense, Dropout, Input, Flatten, concatenate, Reshape\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger, History, Callback, LambdaCallback\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from matplotlib import pyplot\n",
    "import glob\n",
    "import csv\n",
    "import re\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ShuffleData(x_order_addr, y_order_addr):\n",
    "    x_order = np.load(x_order_addr)\n",
    "    y_order = np.load(y_order_addr)\n",
    "    idx_shuffle = np.array(range(x_order.shape[0]))\n",
    "    np.random.shuffle(idx_shuffle)\n",
    "    x_shuffle = x_order[idx_shuffle]\n",
    "    y_shuffle = y_order[idx_shuffle]\n",
    "    return x_shuffle, y_shuffle\n",
    "\n",
    "def DataTwoStream(x_total, y_total, val_ratio = 0.2):\n",
    "    index_split = int(x_total.shape[0]*0.8)\n",
    "    x_train_a = x_total[:index_split,:,0]\n",
    "    x_train_a = x_train_a.reshape((x_train_a.shape[0],x_train_a.shape[1],1))\n",
    "    x_train_b = x_total[:index_split,:,1]\n",
    "    x_train_b =x_train_b.reshape((x_train_a.shape[0],x_train_a.shape[1],1))\n",
    "    y_train = y_total[:index_split]\n",
    "    x_val_a = x_total[index_split:,:,0]\n",
    "    x_val_a = x_val_a.reshape((x_val_a.shape[0],x_val_a.shape[1],1))\n",
    "    x_val_b = x_total[index_split:,:,1]\n",
    "    x_val_b = x_val_b.reshape((x_val_a.shape[0],x_val_a.shape[1],1))\n",
    "    y_val = y_total[index_split:]\n",
    "    return x_train_a, x_train_b, y_train, x_val_a, x_val_b, y_val\n",
    "\n",
    "def GetBinData(class_name):\n",
    "    file_list = os.listdir(os.path.join(os.getcwd(), \"data\", \"npy\", \"bin_order\"))\n",
    "    \n",
    "    for i in range(len(file_list)):\n",
    "        re_x = class_name + '.*(?<!y\\.npy)$'\n",
    "        re_y = class_name + '(.+?)y.npy'\n",
    "        mx = re.search(re_x, file_list[i])\n",
    "        my = re.search(re_y, file_list[i])\n",
    "        if mx:\n",
    "            filex_idx = i\n",
    "        if my:\n",
    "            filey_idx = i\n",
    "    \n",
    "    x_order_addr = os.path.join(os.getcwd(), \"data\", \"npy\", \"bin_order\", file_list[filex_idx])\n",
    "    y_order_addr = os.path.join(os.getcwd(), \"data\", \"npy\", \"bin_order\", file_list[filey_idx])\n",
    "    x_shuffle, y_shuffle = ShuffleData(x_order_addr, y_order_addr)\n",
    "    x_train_a, x_train_b, y_train, x_val_a, x_val_b, y_val = DataTwoStream(x_shuffle, y_shuffle)\n",
    "    class_name = file_list[i][:-8]\n",
    "    return x_train_a, x_train_b, y_train, x_val_a, x_val_b, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I only want to train the last layer, I freeze all the pretrained weights in the 5-classes model which is called base_model here. (based_model + dense) is called head_model here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 60, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 60, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_21 (LSTM)                  (None, 60, 32)       4352        input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_23 (LSTM)                  (None, 60, 32)       4352        input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 60, 32)       0           lstm_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 60, 32)       0           lstm_23[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_22 (LSTM)                  (None, 32)           8320        dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_24 (LSTM)                  (None, 32)           8320        dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 32)           0           lstm_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 32)           0           lstm_24[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 64)           0           dropout_22[0][0]                 \n",
      "                                                                 dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 64)           4160        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 5)            325         dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 5)            0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 5)            20          dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 64)           384         batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 64)           0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64)           256         dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_-1 (Dense)                (None, 1)            65          batch_normalization_14[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 30,554\n",
      "Trainable params: 587\n",
      "Non-trainable params: 29,967\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yue Ma\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:19: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "x_train_a, x_train_b, y_train, x_val_a, x_val_b, y_val = GetBinData('A4')\n",
    "\n",
    "n_classes = 2\n",
    "Initializer=keras.initializers.glorot_normal(seed=None)\n",
    "optimizer = RMSprop(lr=0.0001, rho=0.9, epsilon=1e-6)\n",
    "json_file = open('model_5classes_dense64.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "base_model = model_from_json(loaded_model_json)\n",
    "base_model.load_weights(\"471-0.907-5classes-dense64.hdf5\")\n",
    "######################################## New Model architecture ##############################################\n",
    "x = base_model.output\n",
    "x = Dropout(0.2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(64, activation='relu', kernel_initializer = Initializer)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "predictions = Dense(1, activation = 'sigmoid', kernel_initializer = Initializer, name='dense_-1')(x)\n",
    "head_model = Model(input = base_model.input, output = predictions)\n",
    "#for layer in base_model.layers:\n",
    "    #layer.trainable = False\n",
    "    \n",
    "head_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "head_model.summary()\n",
    "##################################### Helpers in callbacks ##############################################\n",
    "tb = TensorBoard(log_dir=os.path.join('tensorboard', 'logs'))\n",
    "early_stopper = EarlyStopping(patience=20)\n",
    "csv_logger = CSVLogger(os.path.join('logs', str(time.time()) + '.log'))\n",
    "checkpointer = ModelCheckpoint(filepath=os.path.join('checkpoints','{epoch:03d}-{val_acc:.3f}.hdf5'),\n",
    "                                verbose=1,save_best_only=True)\n",
    "history = History()\n",
    "callback = Callback()\n",
    "print_weights1 = LambdaCallback(on_epoch_end=lambda batch, logs: print('layer-1', head_model.layers[-1].get_weights()))\n",
    "#########################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1802 samples, validate on 451 samples\n",
      "Epoch 1/2000\n",
      "1802/1802 [==============================] - 5s 3ms/step - loss: 0.4443 - acc: 0.8008 - val_loss: 0.1988 - val_acc: 0.9446\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.19885, saving model to checkpoints\\001-0.945.hdf5\n",
      "Epoch 2/2000\n",
      "1802/1802 [==============================] - 1s 673us/step - loss: 0.3374 - acc: 0.8452 - val_loss: 0.1685 - val_acc: 0.9468\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.19885 to 0.16850, saving model to checkpoints\\002-0.947.hdf5\n",
      "Epoch 3/2000\n",
      "1802/1802 [==============================] - 1s 608us/step - loss: 0.3049 - acc: 0.8529 - val_loss: 0.1572 - val_acc: 0.9468\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.16850 to 0.15721, saving model to checkpoints\\003-0.947.hdf5\n",
      "Epoch 4/2000\n",
      "1802/1802 [==============================] - 1s 613us/step - loss: 0.3039 - acc: 0.8651 - val_loss: 0.1582 - val_acc: 0.9424\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.15721\n",
      "Epoch 5/2000\n",
      "1802/1802 [==============================] - 1s 644us/step - loss: 0.3320 - acc: 0.8452 - val_loss: 0.1593 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.15721\n",
      "Epoch 6/2000\n",
      "1802/1802 [==============================] - 1s 672us/step - loss: 0.3012 - acc: 0.8674 - val_loss: 0.1605 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.15721\n",
      "Epoch 7/2000\n",
      "1802/1802 [==============================] - 1s 644us/step - loss: 0.2772 - acc: 0.8729 - val_loss: 0.1591 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.15721\n",
      "Epoch 8/2000\n",
      "1802/1802 [==============================] - 2s 1ms/step - loss: 0.2701 - acc: 0.8840 - val_loss: 0.1661 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.15721\n",
      "Epoch 9/2000\n",
      "1802/1802 [==============================] - 2s 1ms/step - loss: 0.2789 - acc: 0.8762 - val_loss: 0.1630 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.15721\n",
      "Epoch 10/2000\n",
      "1802/1802 [==============================] - 2s 844us/step - loss: 0.2935 - acc: 0.8735 - val_loss: 0.1606 - val_acc: 0.9357\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.15721\n",
      "Epoch 11/2000\n",
      "1802/1802 [==============================] - 2s 871us/step - loss: 0.2715 - acc: 0.8840 - val_loss: 0.1644 - val_acc: 0.9335\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.15721\n",
      "Epoch 12/2000\n",
      "1802/1802 [==============================] - 1s 808us/step - loss: 0.2609 - acc: 0.8812 - val_loss: 0.1587 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.15721\n",
      "Epoch 13/2000\n",
      "1802/1802 [==============================] - 1s 788us/step - loss: 0.2805 - acc: 0.8829 - val_loss: 0.1571 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.15721 to 0.15710, saving model to checkpoints\\013-0.938.hdf5\n",
      "Epoch 14/2000\n",
      "1802/1802 [==============================] - 1s 761us/step - loss: 0.2867 - acc: 0.8746 - val_loss: 0.1578 - val_acc: 0.9335\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.15710\n",
      "Epoch 15/2000\n",
      "1802/1802 [==============================] - 1s 758us/step - loss: 0.2925 - acc: 0.8674 - val_loss: 0.1576 - val_acc: 0.9335\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.15710\n",
      "Epoch 16/2000\n",
      "1802/1802 [==============================] - 1s 784us/step - loss: 0.2623 - acc: 0.8846 - val_loss: 0.1600 - val_acc: 0.9335\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.15710\n",
      "Epoch 17/2000\n",
      "1802/1802 [==============================] - 1s 805us/step - loss: 0.2653 - acc: 0.8762 - val_loss: 0.1577 - val_acc: 0.9357\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.15710\n",
      "Epoch 18/2000\n",
      "1802/1802 [==============================] - 1s 771us/step - loss: 0.2783 - acc: 0.8746 - val_loss: 0.1561 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.15710 to 0.15609, saving model to checkpoints\\018-0.938.hdf5\n",
      "Epoch 19/2000\n",
      "1802/1802 [==============================] - 2s 861us/step - loss: 0.2671 - acc: 0.8912 - val_loss: 0.1564 - val_acc: 0.9357\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.15609\n",
      "Epoch 20/2000\n",
      "1802/1802 [==============================] - 1s 825us/step - loss: 0.2727 - acc: 0.8785 - val_loss: 0.1563 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.15609\n",
      "Epoch 21/2000\n",
      "1802/1802 [==============================] - 2s 1ms/step - loss: 0.2609 - acc: 0.8885 - val_loss: 0.1563 - val_acc: 0.9357\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.15609\n",
      "Epoch 22/2000\n",
      "1802/1802 [==============================] - 2s 1ms/step - loss: 0.2725 - acc: 0.8790 - val_loss: 0.1585 - val_acc: 0.9335\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.15609\n",
      "Epoch 23/2000\n",
      "1802/1802 [==============================] - 2s 1ms/step - loss: 0.2677 - acc: 0.8818 - val_loss: 0.1586 - val_acc: 0.9335\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.15609\n",
      "Epoch 24/2000\n",
      "1802/1802 [==============================] - 3s 2ms/step - loss: 0.2653 - acc: 0.8846 - val_loss: 0.1625 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.15609\n",
      "Epoch 25/2000\n",
      "1802/1802 [==============================] - 2s 937us/step - loss: 0.2819 - acc: 0.8735 - val_loss: 0.1620 - val_acc: 0.9335\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.15609\n",
      "Epoch 26/2000\n",
      "1802/1802 [==============================] - 2s 1ms/step - loss: 0.2557 - acc: 0.8901 - val_loss: 0.1580 - val_acc: 0.9335\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.15609\n",
      "Epoch 27/2000\n",
      "1802/1802 [==============================] - 2s 936us/step - loss: 0.2681 - acc: 0.8735 - val_loss: 0.1567 - val_acc: 0.9357\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.15609\n",
      "Epoch 28/2000\n",
      "1802/1802 [==============================] - 2s 943us/step - loss: 0.2634 - acc: 0.8879 - val_loss: 0.1558 - val_acc: 0.9357\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.15609 to 0.15583, saving model to checkpoints\\028-0.936.hdf5\n",
      "Epoch 29/2000\n",
      "1802/1802 [==============================] - 1s 641us/step - loss: 0.2594 - acc: 0.8885 - val_loss: 0.1544 - val_acc: 0.9357\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.15583 to 0.15440, saving model to checkpoints\\029-0.936.hdf5\n",
      "Epoch 30/2000\n",
      "1802/1802 [==============================] - 1s 731us/step - loss: 0.2741 - acc: 0.8818 - val_loss: 0.1595 - val_acc: 0.9335\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.15440\n",
      "Epoch 31/2000\n",
      "1802/1802 [==============================] - 2s 900us/step - loss: 0.2553 - acc: 0.8862 - val_loss: 0.1600 - val_acc: 0.9335\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.15440\n",
      "Epoch 32/2000\n",
      "1802/1802 [==============================] - 2s 842us/step - loss: 0.2580 - acc: 0.8896 - val_loss: 0.1627 - val_acc: 0.9335\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.15440\n",
      "Epoch 33/2000\n",
      "1802/1802 [==============================] - 2s 869us/step - loss: 0.2587 - acc: 0.8868 - val_loss: 0.1528 - val_acc: 0.9357\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.15440 to 0.15280, saving model to checkpoints\\033-0.936.hdf5\n",
      "Epoch 34/2000\n",
      "1802/1802 [==============================] - 1s 760us/step - loss: 0.2755 - acc: 0.8751 - val_loss: 0.1585 - val_acc: 0.9335\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.15280\n",
      "Epoch 35/2000\n",
      "1802/1802 [==============================] - 1s 768us/step - loss: 0.2663 - acc: 0.8790 - val_loss: 0.1580 - val_acc: 0.9335\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.15280\n",
      "Epoch 36/2000\n",
      "1802/1802 [==============================] - 1s 769us/step - loss: 0.2494 - acc: 0.8984 - val_loss: 0.1589 - val_acc: 0.9335\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.15280\n",
      "Epoch 37/2000\n",
      "1802/1802 [==============================] - 1s 794us/step - loss: 0.2588 - acc: 0.8923 - val_loss: 0.1600 - val_acc: 0.9357\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.15280\n",
      "Epoch 38/2000\n",
      "1802/1802 [==============================] - 1s 774us/step - loss: 0.2529 - acc: 0.8912 - val_loss: 0.1576 - val_acc: 0.9335\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.15280\n",
      "Epoch 39/2000\n",
      "1802/1802 [==============================] - 1s 831us/step - loss: 0.2687 - acc: 0.8907 - val_loss: 0.1581 - val_acc: 0.9335\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.15280\n",
      "Epoch 40/2000\n",
      "1802/1802 [==============================] - 1s 770us/step - loss: 0.2740 - acc: 0.8896 - val_loss: 0.1657 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.15280\n",
      "Epoch 41/2000\n",
      "1802/1802 [==============================] - 1s 779us/step - loss: 0.2591 - acc: 0.8923 - val_loss: 0.1677 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.15280\n",
      "Epoch 42/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1802/1802 [==============================] - 1s 756us/step - loss: 0.2507 - acc: 0.9040 - val_loss: 0.1644 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.15280\n",
      "Epoch 43/2000\n",
      "1802/1802 [==============================] - 1s 759us/step - loss: 0.2466 - acc: 0.8868 - val_loss: 0.1679 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.15280\n",
      "Epoch 44/2000\n",
      "1802/1802 [==============================] - 1s 768us/step - loss: 0.2629 - acc: 0.8912 - val_loss: 0.1678 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.15280\n",
      "Epoch 45/2000\n",
      "1802/1802 [==============================] - 1s 766us/step - loss: 0.2412 - acc: 0.8973 - val_loss: 0.1599 - val_acc: 0.9335\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.15280\n",
      "Epoch 46/2000\n",
      "1802/1802 [==============================] - 1s 755us/step - loss: 0.2528 - acc: 0.8979 - val_loss: 0.1606 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.15280\n",
      "Epoch 47/2000\n",
      "1802/1802 [==============================] - 1s 775us/step - loss: 0.2710 - acc: 0.8835 - val_loss: 0.1580 - val_acc: 0.9357\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.15280\n",
      "Epoch 48/2000\n",
      "1802/1802 [==============================] - 1s 810us/step - loss: 0.2525 - acc: 0.9001 - val_loss: 0.1632 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.15280\n",
      "Epoch 49/2000\n",
      "1802/1802 [==============================] - 1s 661us/step - loss: 0.2734 - acc: 0.8812 - val_loss: 0.1673 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.15280\n",
      "Epoch 50/2000\n",
      "1802/1802 [==============================] - 1s 719us/step - loss: 0.2831 - acc: 0.8774 - val_loss: 0.1645 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.15280\n",
      "Epoch 51/2000\n",
      "1802/1802 [==============================] - 2s 1ms/step - loss: 0.2495 - acc: 0.8885 - val_loss: 0.1617 - val_acc: 0.9357\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.15280\n",
      "Epoch 52/2000\n",
      "1802/1802 [==============================] - 2s 965us/step - loss: 0.2544 - acc: 0.8923 - val_loss: 0.1596 - val_acc: 0.9357\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.15280\n",
      "Epoch 53/2000\n",
      "1802/1802 [==============================] - 1s 680us/step - loss: 0.2412 - acc: 0.8957 - val_loss: 0.1602 - val_acc: 0.9335\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.15280\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "hist = head_model.fit([x_train_a, x_train_b], y_train,\n",
    "            batch_size=64, epochs=2000, \n",
    "            validation_data=([x_val_a, x_val_b], y_val),\n",
    "            callbacks=[tb, early_stopper, csv_logger, checkpointer, history, callback])\n",
    "\n",
    "   \n",
    "model_json = head_model.to_json()\n",
    "with open(\"model_head.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "head_model.save_weights(\"model_head.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing should be noted, since my last layer has only one node, the activation function should be sigmoid instead of softmax, this has been pointed out by Sergey O on github issue. Thanks a lot!!\n",
    "\n",
    "So I tried the performence with both one node and two nodes, below are the results. One node with sigmoid function is much better in my case.\n",
    "\n",
    "#### Setup 1:\n",
    "* One node in last layer\n",
    "* activation function of last layer = 'sigmoid'\n",
    "* loss='binary_crossentropy'\n",
    "* all parameters trainable\n",
    "\n",
    "#### Result:\n",
    "* val_loss: 0.14179\n",
    "* val_acc: 0.9534\n",
    "* epochs: 50\n",
    "    \n",
    "#### Setup 2:\n",
    "* One node in last layer\n",
    "* activation function of last layer = 'sigmoid'\n",
    "* loss='binary_crossentropy'\n",
    "* only newly added layers trainable(last 2 layers)\n",
    "\n",
    "#### Result:\n",
    "* val_loss: 0.13604\n",
    "* val_acc: 0.9512\n",
    "* eopchs: 73\n",
    "\n",
    "#### Setup 3:\n",
    "* Two nodes in last layer\n",
    "* activation function of last layer = 'softmax'\n",
    "* loss='sparse_categorical_crossentropy'\n",
    "* all parameters trainable\n",
    "\n",
    "#### Result:\n",
    "* val_loss: 0.1888\n",
    "* val_acc: 0.9379\n",
    "* eopchs: 29\n",
    "\n",
    "#### Setup 4:\n",
    "* Two nodes in last layer\n",
    "* activation function of last layer = 'softmax'\n",
    "* loss='sparse_categorical_crossentropy'\n",
    "* only newly added layers trainable(last 2 layers)\n",
    "\n",
    "#### Result:\n",
    "* val_loss: 0.1305\n",
    "* val_acc: 0.9534\n",
    "* eopchs: \n",
    "\n",
    "For those who don't have a solid foundation knowledge as me, the softmax function and sigmoid function are shown as below. You can see when k(number of classes) = 1, softmax has only one value in denominator, so the total value is always 1. While sigmoid function value is different since there are something else on the denominator. \n",
    "\n",
    "<img src=\"http://i68.tinypic.com/2yvtduv.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x25709bb93c8>,\n",
       " <keras.engine.input_layer.InputLayer at 0x25709bb9e80>,\n",
       " <keras.layers.recurrent.LSTM at 0x25709bb9d30>,\n",
       " <keras.layers.recurrent.LSTM at 0x25709bb9cc0>,\n",
       " <keras.layers.core.Dropout at 0x25709ba1080>,\n",
       " <keras.layers.core.Dropout at 0x25709ba1710>,\n",
       " <keras.layers.recurrent.LSTM at 0x25709ba1588>,\n",
       " <keras.layers.recurrent.LSTM at 0x25709ba1128>,\n",
       " <keras.layers.core.Dropout at 0x25709ba12e8>,\n",
       " <keras.layers.core.Dropout at 0x25709ba1438>,\n",
       " <keras.layers.merge.Concatenate at 0x25709ba1208>,\n",
       " <keras.layers.core.Dense at 0x25709ba17b8>,\n",
       " <keras.layers.core.Dense at 0x25709ba1908>,\n",
       " <keras.layers.core.Dropout at 0x25709b74160>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x2570674dfd0>,\n",
       " <keras.layers.core.Dense at 0x25709f7d4a8>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yue Ma\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:19: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The name \"dropout_21\" is used 2 times in the model. All layer names should be unique.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-9610d2a04c12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInitializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'dense_-1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mhead_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[0;32m     92\u001b[0m             \u001b[1;31m# Graph network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[1;31m# Subclassed network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[1;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[1;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         nodes, nodes_by_depth, layers, layers_by_depth = _map_graph_network(\n\u001b[1;32m--> 237\u001b[1;33m             self.inputs, self.outputs)\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[1;34m(inputs, outputs)\u001b[0m\n\u001b[0;32m   1440\u001b[0m             raise ValueError('The name \"' + name + '\" is used ' +\n\u001b[0;32m   1441\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1442\u001b[1;33m                              \u001b[1;34m' times in the model. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1443\u001b[0m                              'All layer names should be unique.')\n\u001b[0;32m   1444\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnetwork_nodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers_by_depth\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The name \"dropout_21\" is used 2 times in the model. All layer names should be unique."
     ]
    }
   ],
   "source": [
    "x_train_a, x_train_b, y_train, x_val_a, x_val_b, y_val = GetBinData('A4')\n",
    "\n",
    "n_classes = 2\n",
    "Initializer=keras.initializers.glorot_normal(seed=None)\n",
    "optimizer = RMSprop(lr=0.0001, rho=0.9, epsilon=1e-6)\n",
    "json_file = open('model_5classes_dense64.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "base_model = model_from_json(loaded_model_json)\n",
    "base_model.load_weights(\"471-0.907-5classes-dense64.hdf5\")\n",
    "######################################## New Model architecture ##############################################\n",
    "x = base_model.output\n",
    "x = Dropout(0.2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(64, activation='relu', kernel_initializer = Initializer, name='dense_-2')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "predictions = Dense(2, activation = 'softmax', kernel_initializer = Initializer, name='dense_-1')(x)\n",
    "head_model = Model(input = base_model.input, output = predictions)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "head_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "head_model.summary()\n",
    "##################################### Helpers in callbacks ##############################################\n",
    "tb = TensorBoard(log_dir=os.path.join('tensorboard', 'logs'))\n",
    "early_stopper = EarlyStopping(patience=20)\n",
    "csv_logger = CSVLogger(os.path.join('logs', str(time.time()) + '.log'))\n",
    "checkpointer = ModelCheckpoint(filepath=os.path.join('checkpoints','{epoch:03d}-{val_acc:.3f}.hdf5'),\n",
    "                                verbose=1,save_best_only=True)\n",
    "history = History()\n",
    "callback = Callback()\n",
    "print_weights1 = LambdaCallback(on_epoch_end=lambda batch, logs: print('layer-1', head_model.layers[-1].get_weights()))\n",
    "#########################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = head_model.fit([x_train_a, x_train_b], y_train,\n",
    "            batch_size=64, epochs=2000, \n",
    "            validation_data=([x_val_a, x_val_b], y_val),\n",
    "            callbacks=[tb, early_stopper, csv_logger, checkpointer, history, callback])\n",
    "\n",
    "   \n",
    "model_json = head_model.to_json()\n",
    "with open(\"model_head.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "head_model.save_weights(\"model_head.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This base_model for the binary classfication is of the architecture below (function TwoStreamLSTM, it is based on the example of 'shared layers' on Keras functional API guide page (https://keras.io/getting-started/functional-api-guide/)\n",
    "\n",
    "There are more details on this whole tasks:\n",
    "I have 5 classes of bacteria trajectory data. The trajectory data is 60 time steps with 2 features each step, but I know those two features are not closely related (one is change of speed, another is change of angular speed), so I used this TwoStream structure where you basicall process 2 features separately then merge the results togeter by using shared layer. This works well for 5-class classification. \n",
    "\n",
    "But, as biology experimentalists, we are more interested in interpretation rather than just predication. In this 5 classes, there is 1 class is wild type which means \"natural one\", the other 4 are genetically mutant. What researchers are more concerned with is to compare each mutant with the wild type. Therefore, I did binary classification between each mutant and the wild type again with same architecture just different number of classes and therefore the number of nodes of last layer is different. \n",
    "\n",
    "To my suprise, the accuracy of binary classification is lower than the 5-class tasks. I searched a bit and found out it does happen that multi-classes classification accuracy is higher than binary classification with same data. This may due to more features from more classes and the model is forced to pay more attention to the features. \n",
    "\n",
    "So I am thinking since the trained model for 5-classes already worked well (90-93%, genetically mutant doesn't mean necessarily to behave differently, that is, their trajectory data can be non-differentiable), so I decided to add one more layer on the top of the pretrained 5-classes model so that the model can be turned into a binary task. \n",
    "\n",
    "Then the problem happened.....I printed the weights and biases of each epoch, it just never changes...I read all related topic on github issues and stachexchange stc, so far I have tried the following solutions, but none of them works:\n",
    "1. normalize the input and each layer\n",
    "2. make sure the data both input and output are shuffled as expected.\n",
    "3. different optimizer with default parameters' values.\n",
    "4. sgd with different scales of learning rates\n",
    "5. different initializer.\n",
    "\n",
    "Some more notes:\n",
    "1. it doesn't matter if I have the one more dense layer before the softmax dense layer, the weights won't change.\n",
    "2. the number of epoch in model.fit was 2000, since the weights never change, I set it to 10 to terminate it faster. \n",
    "\n",
    "To reproduce this results, you may need to use or read:\n",
    "1. data files in ./data/nyp/bin_order/A1_808_bin.npy and ./data/nyp/bin_order/A1_808_bin_y.npy\n",
    "2. Steps in TwoStreamLSTM I used for 5-classes model or base model\n",
    "3. the pretrained model(model_5classes_dense64.json), weights(471-0.907-5classes-dense64.hdf5) or model with weights(model_5classes_dense64.h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TwoStreamLSTM(x_train_a, x_train_b, y_train, x_val_a, x_val_b, y_val):\n",
    "    data_dim = 1\n",
    "    batch_size = 64\n",
    "    timesteps = x_train_a.shape[1] #60\n",
    "    nb_classes = len(np.unique(y_train)) #2\n",
    "\n",
    "    first_input = Input((timesteps, data_dim))\n",
    "    encoder_a = LSTM(32, return_sequences=True,\n",
    "                     batch_input_shape=(batch_size,timesteps, data_dim))(first_input)\n",
    "    encoder_a = Dropout(0.2)(encoder_a)\n",
    "    encoder_a = LSTM(32)(encoder_a)\n",
    "    encoder_a_out = Dropout(0.2)(encoder_a)\n",
    "    model_a = Model(first_input, encoder_a_out)\n",
    "\n",
    "    second_input = Input((timesteps, data_dim))\n",
    "    encoder_b = LSTM(32, return_sequences=True,\n",
    "                     batch_input_shape=(batch_size, timesteps, data_dim))(second_input)\n",
    "    encoder_b = Dropout(0.2)(encoder_b)\n",
    "    encoder_b = LSTM(16)(encoder_b)\n",
    "    encoder_b_out = Dropout(0.2)(encoder_b)\n",
    "    model_b = Model(second_input, encoder_b_out)\n",
    "\n",
    "    concatenated = concatenate([encoder_a_out, encoder_b_out])\n",
    "    decoder = Dense(8, activation='relu')(concatenated)\n",
    "    output_layer = Dense(nb_classes, activation='softmax')(decoder)\n",
    "\n",
    "    model = Model([first_input, second_input], output_layer)\n",
    "    model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "    ##################################### Helpers in callbacks ##############################################\n",
    "    tb = TensorBoard(log_dir=os.path.join('tensorboard', 'logs',))\n",
    "    early_stopper = EarlyStopping(patience=20)\n",
    "    csv_logger = CSVLogger(os.path.join('logs', str(time.time()) + '.log'))\n",
    "    checkpointer = ModelCheckpoint(filepath=os.path.join('checkpoints','{epoch:03d}-{val_sparse_categorical_accuracy:.3f}.hdf5'),\n",
    "                                    verbose=1,save_best_only=True)\n",
    "    history = History()\n",
    "    #########################################################################################################\n",
    "    hist = model.fit([x_train_a, x_train_b], y_train,\n",
    "                batch_size=batch_size, epochs=2000, \n",
    "                validation_data=([x_val_a, x_val_b], y_val),\n",
    "                callbacks=[tb, early_stopper, csv_logger, checkpointer, history])\n",
    "    \n",
    "    #print(model.summary())   \n",
    "    model_json = model.to_json()\n",
    "    with open(\"model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(\"model.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "    return hist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
