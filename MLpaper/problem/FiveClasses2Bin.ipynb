{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import LSTM, Dense, Dropout, Input, Flatten, concatenate, Reshape\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger, History, Callback, LambdaCallback\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from matplotlib import pyplot\n",
    "import glob\n",
    "import csv\n",
    "import re\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ShuffleData(x_order_addr, y_order_addr):\n",
    "    x_order = np.load(x_order_addr)\n",
    "    y_order = np.load(y_order_addr)\n",
    "    idx_shuffle = np.array(range(x_order.shape[0]))\n",
    "    np.random.shuffle(idx_shuffle)\n",
    "    x_shuffle = x_order[idx_shuffle]\n",
    "    y_shuffle = y_order[idx_shuffle]\n",
    "    return x_shuffle, y_shuffle\n",
    "\n",
    "def DataTwoStream(x_total, y_total, val_ratio = 0.2):\n",
    "    index_split = int(x_total.shape[0]*0.8)\n",
    "    x_train_a = x_total[:index_split,:,0]\n",
    "    x_train_a = x_train_a.reshape((x_train_a.shape[0],x_train_a.shape[1],1))\n",
    "    x_train_b = x_total[:index_split,:,1]\n",
    "    x_train_b =x_train_b.reshape((x_train_a.shape[0],x_train_a.shape[1],1))\n",
    "    y_train = y_total[:index_split]\n",
    "    x_val_a = x_total[index_split:,:,0]\n",
    "    x_val_a = x_val_a.reshape((x_val_a.shape[0],x_val_a.shape[1],1))\n",
    "    x_val_b = x_total[index_split:,:,1]\n",
    "    x_val_b = x_val_b.reshape((x_val_a.shape[0],x_val_a.shape[1],1))\n",
    "    y_val = y_total[index_split:]\n",
    "    return x_train_a, x_train_b, y_train, x_val_a, x_val_b, y_val\n",
    "\n",
    "def GetBinData(class_name):\n",
    "    file_list = os.listdir(os.path.join(os.getcwd(), \"data\", \"npy\", \"bin_order\"))\n",
    "    \n",
    "    for i in range(len(file_list)):\n",
    "        re_x = class_name + '.*(?<!y\\.npy)$'\n",
    "        re_y = class_name + '(.+?)y.npy'\n",
    "        mx = re.search(re_x, file_list[i])\n",
    "        my = re.search(re_y, file_list[i])\n",
    "        if mx:\n",
    "            filex_idx = i\n",
    "        if my:\n",
    "            filey_idx = i\n",
    "    \n",
    "    x_order_addr = os.path.join(os.getcwd(), \"data\", \"npy\", \"bin_order\", file_list[filex_idx])\n",
    "    y_order_addr = os.path.join(os.getcwd(), \"data\", \"npy\", \"bin_order\", file_list[filey_idx])\n",
    "    x_shuffle, y_shuffle = ShuffleData(x_order_addr, y_order_addr)\n",
    "    x_train_a, x_train_b, y_train, x_val_a, x_val_b, y_val = DataTwoStream(x_shuffle, y_shuffle)\n",
    "    class_name = file_list[i][:-8]\n",
    "    return x_train_a, x_train_b, y_train, x_val_a, x_val_b, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I only want to train the last layer, I freeze all the pretrained weights in the 5-classes model which is called base_model here. (based_model + dense) is called head_model here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 60, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 60, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_21 (LSTM)                  (None, 60, 32)       4352        input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_23 (LSTM)                  (None, 60, 32)       4352        input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 60, 32)       0           lstm_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 60, 32)       0           lstm_23[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_22 (LSTM)                  (None, 32)           8320        dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_24 (LSTM)                  (None, 32)           8320        dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 32)           0           lstm_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 32)           0           lstm_24[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 64)           0           dropout_22[0][0]                 \n",
      "                                                                 dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 64)           4160        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 5)            325         dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 5)            0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 5)            20          dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_-2 (Dense)                (None, 2)            12          batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 2)            0           dense_-2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 2)            8           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_-1 (Dense)                (None, 1)            3           batch_normalization_6[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 29,872\n",
      "Trainable params: 29\n",
      "Non-trainable params: 29,843\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yue Ma\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "x_train_a, x_train_b, y_train, x_val_a, x_val_b, y_val = GetBinData('A4')\n",
    "\n",
    "n_classes = 2\n",
    "#Initializer=keras.initializers.glorot_normal(seed=None)\n",
    "Initializer=keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "#optimizer = RMSprop(lr=0.00001, rho=0.9, epsilon=1e-6)\n",
    "optimizer = SGD(lr=0.00001)\n",
    "json_file = open('model_5classes_dense64.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "base_model = model_from_json(loaded_model_json)\n",
    "base_model.load_weights(\"471-0.907-5classes-dense64.hdf5\")\n",
    "######################################## New Model architecture ##############################################\n",
    "x = base_model.output\n",
    "x = Dropout(0.2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "#x = Dense(2, activation='relu', kernel_initializer = Initializer, name='dense_-2')(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "#x = BatchNormalization()(x)\n",
    "predictions = Dense(1, activation = 'softmax', kernel_initializer = Initializer, name='dense_-1')(x)\n",
    "head_model = Model(input = base_model.input, output = predictions)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "head_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "head_model.summary()\n",
    "##################################### Helpers in callbacks ##############################################\n",
    "tb = TensorBoard(log_dir=os.path.join('tensorboard', 'logs'))\n",
    "early_stopper = EarlyStopping(patience=10)\n",
    "csv_logger = CSVLogger(os.path.join('logs', str(time.time()) + '.log'))\n",
    "checkpointer = ModelCheckpoint(filepath=os.path.join('checkpoints','{epoch:03d}-{val_acc:.3f}.hdf5'),\n",
    "                                verbose=1,save_best_only=True)\n",
    "history = History()\n",
    "callback = Callback()\n",
    "print_weights1 = LambdaCallback(on_epoch_end=lambda batch, logs: print('layer-1', head_model.layers[-1].get_weights()))\n",
    "#########################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1802 samples, validate on 451 samples\n",
      "Epoch 1/2000\n",
      "1802/1802 [==============================] - 6s 3ms/step - loss: 7.0688 - acc: 0.5566 - val_loss: 6.7517 - val_acc: 0.5765\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 6.75165, saving model to checkpoints\\001-0.576.hdf5\n",
      "layer-1 [array([[ 0.04330691],\n",
      "       [-0.04108568]], dtype=float32), array([ 0.], dtype=float32)]\n",
      "Epoch 2/2000\n",
      "1802/1802 [==============================] - 1s 676us/step - loss: 7.0688 - acc: 0.5566 - val_loss: 6.7517 - val_acc: 0.5765\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 6.75165\n",
      "layer-1 [array([[ 0.04330691],\n",
      "       [-0.04108568]], dtype=float32), array([ 0.], dtype=float32)]\n",
      "Epoch 3/2000\n",
      "1802/1802 [==============================] - 1s 711us/step - loss: 7.0688 - acc: 0.5566 - val_loss: 6.7517 - val_acc: 0.5765\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 6.75165\n",
      "layer-1 [array([[ 0.04330691],\n",
      "       [-0.04108568]], dtype=float32), array([ 0.], dtype=float32)]\n",
      "Epoch 4/2000\n",
      "1802/1802 [==============================] - 1s 686us/step - loss: 7.0688 - acc: 0.5566 - val_loss: 6.7517 - val_acc: 0.5765\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 6.75165\n",
      "layer-1 [array([[ 0.04330691],\n",
      "       [-0.04108568]], dtype=float32), array([ 0.], dtype=float32)]\n",
      "Epoch 5/2000\n",
      "1802/1802 [==============================] - 1s 548us/step - loss: 7.0688 - acc: 0.5566 - val_loss: 6.7517 - val_acc: 0.5765\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 6.75165\n",
      "layer-1 [array([[ 0.04330691],\n",
      "       [-0.04108568]], dtype=float32), array([ 0.], dtype=float32)]\n",
      "Epoch 6/2000\n",
      "1802/1802 [==============================] - 1s 513us/step - loss: 7.0688 - acc: 0.5566 - val_loss: 6.7517 - val_acc: 0.5765\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 6.75165\n",
      "layer-1 [array([[ 0.04330691],\n",
      "       [-0.04108568]], dtype=float32), array([ 0.], dtype=float32)]\n",
      "Epoch 7/2000\n",
      "1802/1802 [==============================] - 1s 527us/step - loss: 7.0688 - acc: 0.5566 - val_loss: 6.7517 - val_acc: 0.5765\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 6.75165\n",
      "layer-1 [array([[ 0.04330691],\n",
      "       [-0.04108568]], dtype=float32), array([ 0.], dtype=float32)]\n",
      "Epoch 8/2000\n",
      "1802/1802 [==============================] - 1s 517us/step - loss: 7.0688 - acc: 0.5566 - val_loss: 6.7517 - val_acc: 0.5765\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 6.75165\n",
      "layer-1 [array([[ 0.04330691],\n",
      "       [-0.04108568]], dtype=float32), array([ 0.], dtype=float32)]\n",
      "Epoch 9/2000\n",
      "1802/1802 [==============================] - 1s 522us/step - loss: 7.0688 - acc: 0.5566 - val_loss: 6.7517 - val_acc: 0.5765\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 6.75165\n",
      "layer-1 [array([[ 0.04330691],\n",
      "       [-0.04108568]], dtype=float32), array([ 0.], dtype=float32)]\n",
      "Epoch 10/2000\n",
      "1802/1802 [==============================] - 1s 521us/step - loss: 7.0688 - acc: 0.5566 - val_loss: 6.7517 - val_acc: 0.5765\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 6.75165\n",
      "layer-1 [array([[ 0.04330691],\n",
      "       [-0.04108568]], dtype=float32), array([ 0.], dtype=float32)]\n",
      "Epoch 11/2000\n",
      "1802/1802 [==============================] - 1s 531us/step - loss: 7.0688 - acc: 0.5566 - val_loss: 6.7517 - val_acc: 0.5765\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 6.75165\n",
      "layer-1 [array([[ 0.04330691],\n",
      "       [-0.04108568]], dtype=float32), array([ 0.], dtype=float32)]\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "hist = head_model.fit([x_train_a, x_train_b], y_train,\n",
    "            batch_size=64, epochs=2000, \n",
    "            validation_data=([x_val_a, x_val_b], y_val),\n",
    "            callbacks=[tb, early_stopper, csv_logger, checkpointer, history, callback, print_weights1])\n",
    "\n",
    "   \n",
    "model_json = head_model.to_json()\n",
    "with open(\"model_head.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "head_model.save_weights(\"model_head.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x25709bb93c8>,\n",
       " <keras.engine.input_layer.InputLayer at 0x25709bb9e80>,\n",
       " <keras.layers.recurrent.LSTM at 0x25709bb9d30>,\n",
       " <keras.layers.recurrent.LSTM at 0x25709bb9cc0>,\n",
       " <keras.layers.core.Dropout at 0x25709ba1080>,\n",
       " <keras.layers.core.Dropout at 0x25709ba1710>,\n",
       " <keras.layers.recurrent.LSTM at 0x25709ba1588>,\n",
       " <keras.layers.recurrent.LSTM at 0x25709ba1128>,\n",
       " <keras.layers.core.Dropout at 0x25709ba12e8>,\n",
       " <keras.layers.core.Dropout at 0x25709ba1438>,\n",
       " <keras.layers.merge.Concatenate at 0x25709ba1208>,\n",
       " <keras.layers.core.Dense at 0x25709ba17b8>,\n",
       " <keras.layers.core.Dense at 0x25709ba1908>,\n",
       " <keras.layers.core.Dropout at 0x25709b74160>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x2570674dfd0>,\n",
       " <keras.layers.core.Dense at 0x25709f7d4a8>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[100:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
