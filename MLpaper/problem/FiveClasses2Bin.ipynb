{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import LSTM, Dense, Dropout, Input, Flatten, concatenate, Reshape\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger, History, Callback, LambdaCallback\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from matplotlib import pyplot\n",
    "import glob\n",
    "import csv\n",
    "import re\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ShuffleData(x_order_addr, y_order_addr):\n",
    "    x_order = np.load(x_order_addr)\n",
    "    y_order = np.load(y_order_addr)\n",
    "    idx_shuffle = np.array(range(x_order.shape[0]))\n",
    "    np.random.shuffle(idx_shuffle)\n",
    "    x_shuffle = x_order[idx_shuffle]\n",
    "    y_shuffle = y_order[idx_shuffle]\n",
    "    return x_shuffle, y_shuffle\n",
    "\n",
    "def DataTwoStream(x_total, y_total, val_ratio = 0.2):\n",
    "    index_split = int(x_total.shape[0]*0.8)\n",
    "    x_train_a = x_total[:index_split,:,0]\n",
    "    x_train_a = x_train_a.reshape((x_train_a.shape[0],x_train_a.shape[1],1))\n",
    "    x_train_b = x_total[:index_split,:,1]\n",
    "    x_train_b =x_train_b.reshape((x_train_a.shape[0],x_train_a.shape[1],1))\n",
    "    y_train = y_total[:index_split]\n",
    "    x_val_a = x_total[index_split:,:,0]\n",
    "    x_val_a = x_val_a.reshape((x_val_a.shape[0],x_val_a.shape[1],1))\n",
    "    x_val_b = x_total[index_split:,:,1]\n",
    "    x_val_b = x_val_b.reshape((x_val_a.shape[0],x_val_a.shape[1],1))\n",
    "    y_val = y_total[index_split:]\n",
    "    return x_train_a, x_train_b, y_train, x_val_a, x_val_b, y_val\n",
    "\n",
    "def GetBinData(class_name):\n",
    "    file_list = os.listdir(os.path.join(os.getcwd(), \"data\", \"npy\", \"bin_order\"))\n",
    "    \n",
    "    for i in range(len(file_list)):\n",
    "        re_x = class_name + '.*(?<!y\\.npy)$'\n",
    "        re_y = class_name + '(.+?)y.npy'\n",
    "        mx = re.search(re_x, file_list[i])\n",
    "        my = re.search(re_y, file_list[i])\n",
    "        if mx:\n",
    "            filex_idx = i\n",
    "        if my:\n",
    "            filey_idx = i\n",
    "    \n",
    "    x_order_addr = os.path.join(os.getcwd(), \"data\", \"npy\", \"bin_order\", file_list[filex_idx])\n",
    "    y_order_addr = os.path.join(os.getcwd(), \"data\", \"npy\", \"bin_order\", file_list[filey_idx])\n",
    "    x_shuffle, y_shuffle = ShuffleData(x_order_addr, y_order_addr)\n",
    "    x_train_a, x_train_b, y_train, x_val_a, x_val_b, y_val = DataTwoStream(x_shuffle, y_shuffle)\n",
    "    class_name = file_list[i][:-8]\n",
    "    return x_train_a, x_train_b, y_train, x_val_a, x_val_b, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I only want to train the last layer, I freeze all the pretrained weights in the 5-classes model which is called base_model here. (based_model + dense) is called head_model here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 60, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 60, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_21 (LSTM)                  (None, 60, 32)       4352        input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_23 (LSTM)                  (None, 60, 32)       4352        input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 60, 32)       0           lstm_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 60, 32)       0           lstm_23[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_22 (LSTM)                  (None, 32)           8320        dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_24 (LSTM)                  (None, 32)           8320        dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 32)           0           lstm_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 32)           0           lstm_24[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 64)           0           dropout_22[0][0]                 \n",
      "                                                                 dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 64)           4160        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 5)            325         dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 5)            0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 5)            20          dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_-1 (Dense)                (None, 1)            6           batch_normalization_8[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 29,855\n",
      "Trainable params: 29,845\n",
      "Non-trainable params: 10\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yue Ma\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "x_train_a, x_train_b, y_train, x_val_a, x_val_b, y_val = GetBinData('A4')\n",
    "\n",
    "n_classes = 2\n",
    "Initializer=keras.initializers.glorot_normal(seed=None)\n",
    "#Initializer=keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "#optimizer = RMSprop(lr=0.00001, rho=0.9, epsilon=1e-6)\n",
    "optimizer = SGD(lr=0.00001)\n",
    "json_file = open('model_5classes_dense64.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "base_model = model_from_json(loaded_model_json)\n",
    "base_model.load_weights(\"471-0.907-5classes-dense64.hdf5\")\n",
    "######################################## New Model architecture ##############################################\n",
    "x = base_model.output\n",
    "x = Dropout(0.2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "#x = Dense(2, activation='relu', kernel_initializer = Initializer, name='dense_-2')(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "#x = BatchNormalization()(x)\n",
    "predictions = Dense(1, activation = 'softmax', kernel_initializer = Initializer, name='dense_-1')(x)\n",
    "head_model = Model(input = base_model.input, output = predictions)\n",
    "#for layer in base_model.layers:\n",
    "    #layer.trainable = False\n",
    "    \n",
    "head_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "head_model.summary()\n",
    "##################################### Helpers in callbacks ##############################################\n",
    "tb = TensorBoard(log_dir=os.path.join('tensorboard', 'logs'))\n",
    "early_stopper = EarlyStopping(patience=10)\n",
    "csv_logger = CSVLogger(os.path.join('logs', str(time.time()) + '.log'))\n",
    "checkpointer = ModelCheckpoint(filepath=os.path.join('checkpoints','{epoch:03d}-{val_acc:.3f}.hdf5'),\n",
    "                                verbose=1,save_best_only=True)\n",
    "history = History()\n",
    "callback = Callback()\n",
    "print_weights1 = LambdaCallback(on_epoch_end=lambda batch, logs: print('layer-1', head_model.layers[-1].get_weights()))\n",
    "#########################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1802 samples, validate on 451 samples\n",
      "Epoch 1/2000\n",
      "1802/1802 [==============================] - 13s 7ms/step - loss: 7.0776 - acc: 0.5560 - val_loss: 6.7163 - val_acc: 0.5787\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 6.71630, saving model to checkpoints\\001-0.579.hdf5\n",
      "layer-1 [array([[ 0.83001637],\n",
      "       [ 0.84960645],\n",
      "       [-0.57243991],\n",
      "       [-1.02730703],\n",
      "       [ 0.02720529]], dtype=float32), array([ 0.], dtype=float32)]\n",
      "Epoch 2/2000\n",
      "1802/1802 [==============================] - 5s 3ms/step - loss: 7.0776 - acc: 0.5560 - val_loss: 6.7163 - val_acc: 0.5787\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 6.71630\n",
      "layer-1 [array([[ 0.83001637],\n",
      "       [ 0.84960645],\n",
      "       [-0.57243991],\n",
      "       [-1.02730703],\n",
      "       [ 0.02720529]], dtype=float32), array([ 0.], dtype=float32)]\n",
      "Epoch 3/2000\n",
      "1802/1802 [==============================] - 5s 3ms/step - loss: 7.0776 - acc: 0.5560 - val_loss: 6.7163 - val_acc: 0.5787\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 6.71630\n",
      "layer-1 [array([[ 0.83001637],\n",
      "       [ 0.84960645],\n",
      "       [-0.57243991],\n",
      "       [-1.02730703],\n",
      "       [ 0.02720529]], dtype=float32), array([ 0.], dtype=float32)]\n",
      "Epoch 4/2000\n",
      "1802/1802 [==============================] - 5s 3ms/step - loss: 7.0776 - acc: 0.5560 - val_loss: 6.7163 - val_acc: 0.5787\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 6.71630\n",
      "layer-1 [array([[ 0.83001637],\n",
      "       [ 0.84960645],\n",
      "       [-0.57243991],\n",
      "       [-1.02730703],\n",
      "       [ 0.02720529]], dtype=float32), array([ 0.], dtype=float32)]\n",
      "Epoch 5/2000\n",
      "1802/1802 [==============================] - 5s 3ms/step - loss: 7.0776 - acc: 0.5560 - val_loss: 6.7163 - val_acc: 0.5787\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 6.71630\n",
      "layer-1 [array([[ 0.83001637],\n",
      "       [ 0.84960645],\n",
      "       [-0.57243991],\n",
      "       [-1.02730703],\n",
      "       [ 0.02720529]], dtype=float32), array([ 0.], dtype=float32)]\n",
      "Epoch 6/2000\n",
      "1802/1802 [==============================] - 5s 3ms/step - loss: 7.0776 - acc: 0.5560 - val_loss: 6.7163 - val_acc: 0.5787\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 6.71630\n",
      "layer-1 [array([[ 0.83001637],\n",
      "       [ 0.84960645],\n",
      "       [-0.57243991],\n",
      "       [-1.02730703],\n",
      "       [ 0.02720529]], dtype=float32), array([ 0.], dtype=float32)]\n",
      "Epoch 7/2000\n",
      "1802/1802 [==============================] - 5s 3ms/step - loss: 7.0776 - acc: 0.5560 - val_loss: 6.7163 - val_acc: 0.5787\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 6.71630\n",
      "layer-1 [array([[ 0.83001637],\n",
      "       [ 0.84960645],\n",
      "       [-0.57243991],\n",
      "       [-1.02730703],\n",
      "       [ 0.02720529]], dtype=float32), array([ 0.], dtype=float32)]\n",
      "Epoch 8/2000\n",
      "1802/1802 [==============================] - 6s 3ms/step - loss: 7.0776 - acc: 0.5560 - val_loss: 6.7163 - val_acc: 0.5787\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 6.71630\n",
      "layer-1 [array([[ 0.83001637],\n",
      "       [ 0.84960645],\n",
      "       [-0.57243991],\n",
      "       [-1.02730703],\n",
      "       [ 0.02720529]], dtype=float32), array([ 0.], dtype=float32)]\n",
      "Epoch 9/2000\n",
      "1802/1802 [==============================] - 5s 3ms/step - loss: 7.0776 - acc: 0.5560 - val_loss: 6.7163 - val_acc: 0.5787\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 6.71630\n",
      "layer-1 [array([[ 0.83001637],\n",
      "       [ 0.84960645],\n",
      "       [-0.57243991],\n",
      "       [-1.02730703],\n",
      "       [ 0.02720529]], dtype=float32), array([ 0.], dtype=float32)]\n",
      "Epoch 10/2000\n",
      "1802/1802 [==============================] - 5s 3ms/step - loss: 7.0776 - acc: 0.5560 - val_loss: 6.7163 - val_acc: 0.5787\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 6.71630\n",
      "layer-1 [array([[ 0.83001637],\n",
      "       [ 0.84960645],\n",
      "       [-0.57243991],\n",
      "       [-1.02730703],\n",
      "       [ 0.02720529]], dtype=float32), array([ 0.], dtype=float32)]\n",
      "Epoch 11/2000\n",
      "1802/1802 [==============================] - 5s 3ms/step - loss: 7.0776 - acc: 0.5560 - val_loss: 6.7163 - val_acc: 0.5787\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 6.71630\n",
      "layer-1 [array([[ 0.83001637],\n",
      "       [ 0.84960645],\n",
      "       [-0.57243991],\n",
      "       [-1.02730703],\n",
      "       [ 0.02720529]], dtype=float32), array([ 0.], dtype=float32)]\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "hist = head_model.fit([x_train_a, x_train_b], y_train,\n",
    "            batch_size=64, epochs=2000, \n",
    "            validation_data=([x_val_a, x_val_b], y_val),\n",
    "            callbacks=[tb, early_stopper, csv_logger, checkpointer, history, callback, print_weights1])\n",
    "\n",
    "   \n",
    "model_json = head_model.to_json()\n",
    "with open(\"model_head.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "head_model.save_weights(\"model_head.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x25709bb93c8>,\n",
       " <keras.engine.input_layer.InputLayer at 0x25709bb9e80>,\n",
       " <keras.layers.recurrent.LSTM at 0x25709bb9d30>,\n",
       " <keras.layers.recurrent.LSTM at 0x25709bb9cc0>,\n",
       " <keras.layers.core.Dropout at 0x25709ba1080>,\n",
       " <keras.layers.core.Dropout at 0x25709ba1710>,\n",
       " <keras.layers.recurrent.LSTM at 0x25709ba1588>,\n",
       " <keras.layers.recurrent.LSTM at 0x25709ba1128>,\n",
       " <keras.layers.core.Dropout at 0x25709ba12e8>,\n",
       " <keras.layers.core.Dropout at 0x25709ba1438>,\n",
       " <keras.layers.merge.Concatenate at 0x25709ba1208>,\n",
       " <keras.layers.core.Dense at 0x25709ba17b8>,\n",
       " <keras.layers.core.Dense at 0x25709ba1908>,\n",
       " <keras.layers.core.Dropout at 0x25709b74160>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x2570674dfd0>,\n",
       " <keras.layers.core.Dense at 0x25709f7d4a8>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[100:120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This base_model for the binary classfication is of the architecture below (function TwoStreamLSTM, it is based on the example of 'shared layers' on Keras functional API guide page (https://keras.io/getting-started/functional-api-guide/)\n",
    "\n",
    "There are more details on this whole tasks:\n",
    "I have 5 classes of bacteria trajectory data. The trajectory data is 60 time steps with 2 features each step, but I know those two features are not closely related (one is change of speed, another is change of angular speed), so I used this TwoStream structure where you basicall process 2 features separately then merge the results togeter by using shared layer. This works well for 5-class classification. \n",
    "\n",
    "But, as biology experimentalists, we are more interested in interpretation rather than just predication. In this 5 classes, there is 1 class is wild type which means \"natural one\", the other 4 are genetically mutant. What researchers are more concerned with is to compare each mutant with the wild type. Therefore, I did binary classification between each mutant and the wild type again with same architecture just different number of classes and therefore the number of nodes of last layer is different. \n",
    "\n",
    "To my suprise, the accuracy of binary classification is lower than the 5-class tasks. I searched a bit and found out it does happen that multi-classes classification accuracy is higher than binary classification with same data. This may due to more features from more classes and the model is forced to pay more attention to the features. \n",
    "\n",
    "So I am thinking since the trained model for 5-classes already worked well (90-93%, genetically mutant doesn't mean necessarily to behave differently, that is, their trajectory data can be non-differentiable), so I decided to add one more layer on the top of the pretrained 5-classes model so that the model can be turned into a binary task. \n",
    "\n",
    "Then the problem happened.....I printed the weights and biases of each epoch, it just never changes...I read all related topic on github issues and stachexchange stc, so far I have tried the following solutions, but none of them works:\n",
    "1. normalize the input and each layer\n",
    "2. make sure the data both input and output are shuffled as expected.\n",
    "3. different optimizer with default parameters' values.\n",
    "4. sgd with different scales of learning rates\n",
    "5. different initializer.\n",
    "\n",
    "Some more notes:\n",
    "1. it doesn't matter if I have the one more dense layer before the softmax dense layer, the weights won't change.\n",
    "2. the number of epoch in model.fit was 2000, since the weights never change, I set it to 10 to terminate it faster. \n",
    "\n",
    "To reproduce this results, you may need to use or read:\n",
    "1. data files in ./data/nyp/bin_order/A1_808_bin.npy and ./data/nyp/bin_order/A1_808_bin_y.npy\n",
    "2. Steps in TwoStreamLSTM I used for 5-classes model or base model\n",
    "3. the pretrained model(model_5classes_dense64.json), weights(471-0.907-5classes-dense64.hdf5) or model with weights(model_5classes_dense64.h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TwoStreamLSTM(x_train_a, x_train_b, y_train, x_val_a, x_val_b, y_val):\n",
    "    data_dim = 1\n",
    "    batch_size = 64\n",
    "    timesteps = x_train_a.shape[1] #60\n",
    "    nb_classes = len(np.unique(y_train)) #2\n",
    "\n",
    "    first_input = Input((timesteps, data_dim))\n",
    "    encoder_a = LSTM(32, return_sequences=True,\n",
    "                     batch_input_shape=(batch_size,timesteps, data_dim))(first_input)\n",
    "    encoder_a = Dropout(0.2)(encoder_a)\n",
    "    encoder_a = LSTM(32)(encoder_a)\n",
    "    encoder_a_out = Dropout(0.2)(encoder_a)\n",
    "    model_a = Model(first_input, encoder_a_out)\n",
    "\n",
    "    second_input = Input((timesteps, data_dim))\n",
    "    encoder_b = LSTM(32, return_sequences=True,\n",
    "                     batch_input_shape=(batch_size, timesteps, data_dim))(second_input)\n",
    "    encoder_b = Dropout(0.2)(encoder_b)\n",
    "    encoder_b = LSTM(16)(encoder_b)\n",
    "    encoder_b_out = Dropout(0.2)(encoder_b)\n",
    "    model_b = Model(second_input, encoder_b_out)\n",
    "\n",
    "    concatenated = concatenate([encoder_a_out, encoder_b_out])\n",
    "    decoder = Dense(8, activation='relu')(concatenated)\n",
    "    output_layer = Dense(nb_classes, activation='softmax')(decoder)\n",
    "\n",
    "    model = Model([first_input, second_input], output_layer)\n",
    "    model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "    ##################################### Helpers in callbacks ##############################################\n",
    "    tb = TensorBoard(log_dir=os.path.join('tensorboard', 'logs',))\n",
    "    early_stopper = EarlyStopping(patience=20)\n",
    "    csv_logger = CSVLogger(os.path.join('logs', str(time.time()) + '.log'))\n",
    "    checkpointer = ModelCheckpoint(filepath=os.path.join('checkpoints','{epoch:03d}-{val_sparse_categorical_accuracy:.3f}.hdf5'),\n",
    "                                    verbose=1,save_best_only=True)\n",
    "    history = History()\n",
    "    #########################################################################################################\n",
    "    hist = model.fit([x_train_a, x_train_b], y_train,\n",
    "                batch_size=batch_size, epochs=2000, \n",
    "                validation_data=([x_val_a, x_val_b], y_val),\n",
    "                callbacks=[tb, early_stopper, csv_logger, checkpointer, history])\n",
    "    \n",
    "    #print(model.summary())   \n",
    "    model_json = model.to_json()\n",
    "    with open(\"model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(\"model.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "    return hist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
